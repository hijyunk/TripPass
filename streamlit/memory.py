import streamlit as st
from streamlit_chat import message
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory

def generate_response(prompt):
    try:
        # ëŒ€í™” ë§¥ë½ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ìƒì„±
        if 'memory' not in st.session_state:
            st.session_state.memory = ConversationSummaryBufferMemory(
                llm=ChatOpenAI(model="gpt-3.5-turbo"),
                max_token_limit=1000  # í•„ìš”í•œ í† í° ì œí•œ ì„¤ì •
            )

        # ConversationChainì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        conversation_chain = ConversationChain(
            llm=ChatOpenAI(
                model="gpt-3.5-turbo",
            ),
            memory=st.session_state.memory
        )
        
        response = conversation_chain.run(prompt)
        
        # ëŒ€í™” ìš”ì•½ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ìž…ë ¥ê³¼ ì¶œë ¥ì„ ì €ìž¥
        st.session_state.memory.save_context({"input": prompt}, {"output": response})
        memory_variables = st.session_state.memory.load_memory_variables({})
        
        summary = memory_variables.get('summary', 'No summary available')
        
        return response, summary, memory_variables
    except Exception as e:
        st.error(f"error: {e}")

def show():
    # Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
    st.header("ðŸ¤–ê¸°ì–µí•˜ëŠ” ì„¸ì˜ì´2 (ìš”ì•½ê°€ëŠ¥)")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'generated' not in st.session_state:
        st.session_state['generated'] = []

    if 'past' not in st.session_state:
        st.session_state['past'] = []

    if 'summary' not in st.session_state:
        st.session_state['summary'] = "No summary available"

    # ì‚¬ìš©ìž ìž…ë ¥ì„ ë°›ëŠ” í¼ ì„¤ì •
    with st.form('form', clear_on_submit=True):
        user_input = st.text_input('You: ', '', key='input')
        submitted = st.form_submit_button('Send')

    # ì‚¬ìš©ìž ìž…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
    if submitted and user_input:
        # ì‚¬ìš©ìž ìž…ë ¥ ë¨¼ì € ì¶œë ¥
        st.session_state.past.append(user_input)
        
        # AI ì‘ë‹µ ìƒì„± ë° ìš”ì•½ ì—…ë°ì´íŠ¸
        ai_response, summary, _ = generate_response(user_input)
        st.session_state.generated.append(ai_response or "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.session_state.summary = summary

    # ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì¶œë ¥
    for i in range(len(st.session_state['past']) - 1, -1, -1):
        message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')
        if len(st.session_state['generated']) > i:
            message(st.session_state['generated'][i], key=str(i))

    # ëŒ€í™” ìš”ì•½ ì¶œë ¥
    st.subheader("ëŒ€í™” ìš”ì•½")
    show_debug_button = st.button("ìš”ì•½ ë³´ê¸°")
    if show_debug_button:
        ai_response, summary, memory_variables = generate_response(user_input)
        st.text_area("ë””ë²„ê¹… ì¶œë ¥", value=str(memory_variables), height=200)    